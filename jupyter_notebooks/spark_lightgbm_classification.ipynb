{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import uuid\n",
    "# from spark_helper.core import create_spark_session\n",
    "import psutil\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "\n",
    "# from code_monitor.instrument_code import SystemMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db235bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf953eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/jovyan/data\"\n",
    "RESULTS_DIR = \"/home/jovyan/results/classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the results directory\n",
    "if os.path.exists(RESULTS_DIR):\n",
    "    for item in os.listdir(RESULTS_DIR):\n",
    "        item_path = os.path.join(RESULTS_DIR, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(item_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52680)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 766, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# .config((\"spark.driver.maxResultSize\", \"4g\")) \\\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkLightgbmClassification\") \\\n",
    "    .master(os.environ.get(\"SPARK_MASTER\", \"spark://spark-master:7077\")) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\", \"true\") \\\n",
    "    .config(\"spark.submit.pyFiles\", \"code_monitor.zip\") \\\n",
    "    .getOrCreate()\n",
    "# spark = create_spark_session(\"spark_cluster.yaml\")\n",
    "\n",
    "# get spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a521bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique fold ids from the spark DataFrame\n",
    "fold_ids = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "fold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_model(params=None):\n",
    "    \"\"\"\n",
    "    Train a LightGBM classification model with the specified parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : dict, optional\n",
    "        Parameters for LGBMClassifier. If None, default parameters will be used.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        File path where results are stored\n",
    "    \"\"\"\n",
    "    from code_monitor.instrument_code import SystemMonitor\n",
    "    \n",
    "    print(f\"Training LightGBM classification model...{params}\")\n",
    "\n",
    "    monitor = SystemMonitor()\n",
    "\n",
    "    memory_before_read_mb = monitor.get_current_rss()\n",
    "\n",
    "    # Default parameters if none provided\n",
    "    if params is None:\n",
    "        raise ValueError(\"No parameters provided for LightGBM model.\")\n",
    "    \n",
    "    fold_id = params.pop(\"fold_id\")\n",
    "\n",
    "    # record time to determine how long it takes to read the data\n",
    "    start_time = time.perf_counter()\n",
    "    test_df = pd.read_parquet(\n",
    "        os.path.join(DATA_DIR, f\"ts_fold_{fold_id}_test.parquet\"),\n",
    "    )\n",
    "    train_df = pd.read_parquet(\n",
    "        os.path.join(DATA_DIR, f\"ts_fold_{fold_id}_train.parquet\"),\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    train_test_read_time_sec = end_time - start_time\n",
    "\n",
    "    train_df_mb = train_df.memory_usage(deep=True, index=True).sum() / (1024 * 1024)  # in MB\n",
    "    test_df_mb = test_df.memory_usage(deep=True, index=True).sum() / (1024 * 1024)  # in MB\n",
    "\n",
    "    train_x = train_df.drop(columns=[\"target\", \"date\"])\n",
    "    train_y = train_df[\"target\"]\n",
    "    test_x = test_df.drop(columns=[\"target\", \"date\"])\n",
    "    test_y = test_df[\"target\"]\n",
    "\n",
    "    # get memory usage after reading the data\n",
    "    memory_after_read_mb = monitor.get_current_rss()\n",
    "\n",
    "    # Initialize the classification model\n",
    "    model = LGBMClassifier(verbose=-1, **params)\n",
    "    \n",
    "    # Train the model and measure time\n",
    "    before_cpu_snapshot = monitor.snapshot_cpu()\n",
    "    start_time = time.perf_counter()\n",
    "    model.fit(\n",
    "        train_x, train_y,\n",
    "        eval_set=[(test_x, test_y)],\n",
    "        eval_metric='logloss',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500),\n",
    "            lgb.log_evaluation(period=100),\n",
    "        ]\n",
    "    )\n",
    "    after_cpu_snapshot = monitor.snapshot_cpu()\n",
    "    end_time = time.perf_counter()\n",
    "    training_time = end_time - start_time\n",
    "    fit_cpu_utilization = monitor.compute_cpu_usage(before_cpu_snapshot, after_cpu_snapshot)\n",
    "\n",
    "    # Log memory usage after training\n",
    "    memory_after_training_mb = monitor.get_current_rss()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(test_x)[:, 1]  # Probability of positive class\n",
    "    y_pred = model.predict(test_x)  # Class predictions\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(test_y, y_pred)\n",
    "    precision = precision_score(test_y, y_pred)\n",
    "    recall = recall_score(test_y, y_pred)\n",
    "    f1 = f1_score(test_y, y_pred)\n",
    "    auc = roc_auc_score(test_y, y_pred_proba)\n",
    "\n",
    "    # Store detailed classification report as string\n",
    "    class_report = classification_report(test_y, y_pred)\n",
    "    print(f\"\\nClassification Report for fold {fold_id}:\")\n",
    "    print(class_report)\n",
    "\n",
    "    results = {\n",
    "        \"fold_id\": fold_id,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc,\n",
    "        'training_time': training_time,\n",
    "    }\n",
    "    results.update(params)\n",
    "\n",
    "    # generate uuid string for the results file name\n",
    "    fp_id = str(uuid.uuid4())\n",
    "    fp_name = os.path.join(RESULTS_DIR, f\"results_fold_{fold_id}_{fp_id}.parquet\")\n",
    "\n",
    "    results_df = pd.DataFrame([results])\n",
    "    results_df.to_parquet(\n",
    "        fp_name,\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    gc.collect()  # Force garbage collection to free up memory\n",
    "\n",
    "    return {\"fp_name\": fp_name, \n",
    "            \"fold_id\": fold_id,\n",
    "            \"train_df_mb\": train_df_mb,\n",
    "            \"test_df_mb\": test_df_mb,\n",
    "            \"train_test_read_time_sec\": train_test_read_time_sec,\n",
    "            \"memory_before_read_mb\": memory_before_read_mb,\n",
    "            \"memory_after_read_mb\": memory_after_read_mb,\n",
    "            \"memory_after_training_mb\": memory_after_training_mb,\n",
    "            \"fit_cpu_utilization\": fit_cpu_utilization,\n",
    "            \"training_time\": training_time,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "\n",
    "param_grid = {\n",
    "    \"n_jobs\": [5],\n",
    "    \"fold_id\": fold_ids,\n",
    "    \"lambda_l1\": [0.3, 12, 40],\n",
    "    \"max_depth\": [10, 15, 17],\n",
    "    \"colsample_bytree\": [0.1, 0.3, 0.4],\n",
    "    \"alpha\": [0.2],\n",
    "    \"num_leaves\": [2048,],\n",
    "    \"learning_rate\": [0.03],\n",
    "    \"lambda_l2\": [0.01, 0.1],\n",
    "    \"max_bin\": [256,],\n",
    "    \"bagging_fraction\": [1],\n",
    "    \"deterministic\": [False],\n",
    "    \"objective\": [\"huber\"],\n",
    "    \"metric\": [\"huber\"],\n",
    "    \"n_estimators\": [20000],\n",
    "    \"random_state\": [42],\n",
    "    \"importance_type\": [\"gain\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "param_keys = list(param_grid.keys())\n",
    "param_values = list(param_grid.values())\n",
    "param_combinations = list(itertools.product(*param_values))\n",
    "\n",
    "# Create a list of dictionaries, each representing a specific combination\n",
    "param_dicts = []\n",
    "for combo in param_combinations:\n",
    "    param_dict = dict(zip(param_keys, combo))\n",
    "    param_dicts.append(param_dict)\n",
    "\n",
    "# Display the number of combinations and the first few combinations\n",
    "print(f\"Total number of parameter combinations: {len(param_dicts)}\")\n",
    "print(\"\\nFirst 3 parameter combinations:\")\n",
    "for i in range(min(3, len(param_dicts))):\n",
    "    print(f\"Combination {i+1}:\")\n",
    "    print(param_dicts[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_these = param_dicts[:10] + param_dicts[54:64] + param_dicts[108:118] + param_dicts[162:172] # Limit to the first 10 combinations for testing\n",
    "\n",
    "rdd = sc.parallelize(process_these, numSlices=len(process_these))\n",
    "\n",
    "rdd_result = rdd.map(lambda x: train_lightgbm_model(params=x)).collect()\n",
    "\n",
    "print(f\">>>>length of rdd_result: {len(rdd_result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a126d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas display of results with two decimal places and a comma as a thousands separator\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# Convert the results to a DataFrame and summarize\n",
    "df_results = pd.DataFrame(rdd_result)\n",
    "print(df_results[[\n",
    "    \"fold_id\", \"memory_before_read_mb\", \"memory_after_read_mb\", \"memory_after_training_mb\", \"train_df_mb\", \"test_df_mb\", \"train_test_read_time_sec\", \"training_time\", \"fit_cpu_utilization\",\n",
    "    ]]\\\n",
    "    .groupby(\"fold_id\")\\\n",
    "    .describe(percentiles=[0.5]).T) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d61cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.read_parquet(d[\"fp_name\"]) for d in rdd_result]\n",
    "results_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c97346",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"training_time\", \"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc_roc\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f8ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
