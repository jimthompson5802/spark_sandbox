{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856b9baf",
   "metadata": {},
   "source": [
    "# Spark and Data Science Libraries Sample Notebook\n",
    "\n",
    "This notebook demonstrates how to use PySpark in conjunction with pandas, numpy, scikit-learn and LightGBM for data analysis and machine learning tasks.\n",
    "\n",
    "Created: May 7, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587e574",
   "metadata": {},
   "source": [
    "## 1. Setting up the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf836a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/08 00:29:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/08 00:29:09 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to spark-master/172.18.0.2:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:284)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: spark-master/172.18.0.2:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/08 00:29:29 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to spark-master/172.18.0.2:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:284)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: spark-master/172.18.0.2:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/08 00:29:49 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to spark-master/172.18.0.2:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:284)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:226)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: spark-master/172.18.0.2:7077\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/08 00:30:09 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "25/05/08 00:30:09 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.1\n",
      "Spark UI available at: http://cc3f168ec6f4:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 00:30:10 WARN AbstractConnector: \n",
      "java.io.IOException: Resource temporarily unavailable\n",
      "\tat java.base/sun.nio.ch.NativeThread.signal(Native Method)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.implCloseSelectableChannel(ServerSocketChannelImpl.java:365)\n",
      "\tat java.base/java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:242)\n",
      "\tat java.base/java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:112)\n",
      "\tat org.sparkproject.jetty.server.ServerConnector.close(ServerConnector.java:371)\n",
      "\tat org.sparkproject.jetty.server.AbstractNetworkConnector.shutdown(AbstractNetworkConnector.java:104)\n",
      "\tat org.sparkproject.jetty.server.Server.doStop(Server.java:444)\n",
      "\tat org.sparkproject.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:94)\n",
      "\tat org.apache.spark.ui.ServerInfo.stop(JettyUtils.scala:526)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$stop$2(WebUI.scala:182)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$stop$2$adapted(WebUI.scala:182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ui.WebUI.stop(WebUI.scala:182)\n",
      "\tat org.apache.spark.ui.SparkUI.stop(SparkUI.scala:141)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$7(SparkContext.scala:2118)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$7$adapted(SparkContext.scala:2118)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$6(SparkContext.scala:2118)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2118)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2068)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkDataScienceSample\") \\\n",
    "    .master(os.environ.get(\"SPARK_MASTER\", \"spark://spark-master:7077\")) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa137bb-0941-4941-9028-04f409166f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58896937",
   "metadata": {},
   "source": [
    "## 2. Creating Sample Data with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4be2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import random\n",
    "\n",
    "# Create a schema for our data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"value1\", DoubleType(), False),\n",
    "    StructField(\"value2\", DoubleType(), False),\n",
    "    StructField(\"target\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Generate some random data\n",
    "categories = [\"A\", \"B\", \"C\", \"D\"]\n",
    "data = []\n",
    "\n",
    "for i in range(1000):\n",
    "    category = random.choice(categories)\n",
    "    value1 = random.uniform(0, 100)\n",
    "    value2 = random.uniform(0, 100)\n",
    "    # Simple rule: if value1 > 50 and category is A or B, target is 1, otherwise 0\n",
    "    target = 1 if (value1 > 50 and category in [\"A\", \"B\"]) else 0\n",
    "    data.append(Row(id=i, category=category, value1=value1, value2=value2, target=target))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show a sample of the data\n",
    "df.show(5)\n",
    "print(f\"Total records: {df.count()}\")\n",
    "\n",
    "# Show basic statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76869e2",
   "metadata": {},
   "source": [
    "## 3. Converting Spark DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Display pandas DataFrame info\n",
    "print(\"Pandas DataFrame Information:\")\n",
    "pandas_df.info()\n",
    "\n",
    "# Basic statistics with pandas\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(pandas_df.describe())\n",
    "\n",
    "# Display correlation between numeric columns\n",
    "correlation = pandas_df[[\"value1\", \"value2\", \"target\"]].corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b13e19",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for plots\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a subplot grid\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(pandas_df[\"value1\"], kde=True)\n",
    "plt.title(\"Distribution of value1\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(x=\"category\", data=pandas_df)\n",
    "plt.title(\"Count by Category\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.scatterplot(x=\"value1\", y=\"value2\", hue=\"target\", data=pandas_df)\n",
    "plt.title(\"Scatter Plot of value1 vs value2 by target\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(x=\"category\", y=\"value1\", data=pandas_df)\n",
    "plt.title(\"value1 by Category\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb508a",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some features using numpy and pandas\n",
    "pandas_df[\"value_ratio\"] = pandas_df[\"value1\"] / (pandas_df[\"value2\"] + 1)  # Adding 1 to avoid division by zero\n",
    "pandas_df[\"value_sum\"] = pandas_df[\"value1\"] + pandas_df[\"value2\"]\n",
    "pandas_df[\"value_diff\"] = np.abs(pandas_df[\"value1\"] - pandas_df[\"value2\"])\n",
    "pandas_df[\"value1_log\"] = np.log1p(pandas_df[\"value1\"])  # log1p to avoid issues with zero\n",
    "\n",
    "# Create dummy variables for category\n",
    "category_dummies = pd.get_dummies(pandas_df[\"category\"], prefix=\"category\")\n",
    "pandas_df = pd.concat([pandas_df, category_dummies], axis=1)\n",
    "\n",
    "# Display the DataFrame with new features\n",
    "print(\"DataFrame with new features:\")\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039a84c",
   "metadata": {},
   "source": [
    "## 6. Machine Learning with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d971315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# Select features for model training\n",
    "features = [\"value1\", \"value2\", \"value_ratio\", \"value_sum\", \"value_diff\", \"value1_log\", \n",
    "            \"category_A\", \"category_B\", \"category_C\", \"category_D\"]\n",
    "X = pandas_df[features]\n",
    "y = pandas_df[\"target\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "y_prob = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": rf_model.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance)\n",
    "plt.title(\"Feature Importance from Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b72e4",
   "metadata": {},
   "source": [
    "## 7. Advanced ML with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be587a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create LightGBM datasets\n",
    "lgb_train = lgb.Dataset(X_train_scaled, y_train)\n",
    "lgb_test = lgb.Dataset(X_test_scaled, y_test, reference=lgb_train)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train model\n",
    "print(\"Training LightGBM model...\")\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=100,\n",
    "                      valid_sets=[lgb_train, lgb_test],\n",
    "                      early_stopping_rounds=10)\n",
    "\n",
    "# Make predictions\n",
    "lgb_pred = lgb_model.predict(X_test_scaled)\n",
    "lgb_binary_pred = [1 if pred > 0.5 else 0 for pred in lgb_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nLightGBM Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, lgb_binary_pred):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, lgb_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, lgb_binary_pred))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "lgb.plot_importance(lgb_model, max_num_features=10)\n",
    "plt.title(\"Feature Importance from LightGBM\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6c440",
   "metadata": {},
   "source": [
    "## 8. Going Back to Spark - Saving Results to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to our pandas DataFrame\n",
    "test_indices = pandas_df.iloc[y_test.index].index\n",
    "pandas_df.loc[test_indices, \"rf_prediction\"] = y_pred\n",
    "pandas_df.loc[test_indices, \"rf_probability\"] = y_prob\n",
    "pandas_df.loc[test_indices, \"lgb_prediction\"] = lgb_binary_pred\n",
    "pandas_df.loc[test_indices, \"lgb_probability\"] = lgb_pred\n",
    "\n",
    "# Convert back to a Spark DataFrame\n",
    "result_spark_df = spark.createDataFrame(pandas_df)\n",
    "result_spark_df.show(5)\n",
    "\n",
    "# Register as a temporary view for SQL queries\n",
    "result_spark_df.createOrReplaceTempView(\"model_results\")\n",
    "\n",
    "# Example: Run a Spark SQL query\n",
    "print(\"\\nAnalyzing model results using Spark SQL:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    COUNT(*) as total_count,\n",
    "    SUM(CASE WHEN target = 1 THEN 1 ELSE 0 END) as actual_positive,\n",
    "    SUM(CASE WHEN rf_prediction = 1 THEN 1 ELSE 0 END) as rf_predicted_positive,\n",
    "    SUM(CASE WHEN lgb_prediction = 1 THEN 1 ELSE 0 END) as lgb_predicted_positive\n",
    "FROM model_results\n",
    "GROUP BY category\n",
    "ORDER BY category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ecc52",
   "metadata": {},
   "source": [
    "## 9. Close Spark Session when Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "# Uncomment this when you want to close the Spark session\n",
    "# spark.stop()\n",
    "\n",
    "print(\"Notebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
